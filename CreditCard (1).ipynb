{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Credit Card Fraud Detection with Advanced Pipeline\n",
        "# Handles Imbalanced Data with Isolation Forest + SMOTE\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest, GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, classification_report, roc_curve, auc\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==================== STAGE 1: DATA LOADING & EXPLORATION ====================\n",
        "print(\"=\" * 70)\n",
        "print(\"CREDIT CARD FRAUD DETECTION PIPELINE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('credit_card_data.csv')\n",
        "print(\"\\n[1] Dataset Loaded Successfully\")\n",
        "print(f\"    Shape: {df.shape}\")\n",
        "print(f\"    Columns: {df.columns.tolist()}\")\n",
        "print(f\"\\n    Fraud Distribution:\\n{df['Class'].value_counts()}\")\n",
        "print(f\"    Fraud Percentage: {df['Class'].mean()*100:.2f}%\")\n",
        "\n",
        "# ==================== STAGE 2: PREPROCESSING ====================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[2] DATA PREPROCESSING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Handle missing values - Ensure no NaNs in 'Class' column\n",
        "df.dropna(subset=['Class'], inplace=True)\n",
        "print(f\"    Missing values (after dropping NaNs in 'Class'): {df.isnull().sum().sum()}\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "print(f\"    ‚úì Features scaled using StandardScaler\")\n",
        "\n",
        "# ==================== STAGE 3: UNIQUE FEATURE - ISOLATION FOREST ====================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[3] ANOMALY DETECTION (Isolation Forest)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "iso_forest = IsolationForest(\n",
        "    contamination=0.01,  # Expected fraud rate\n",
        "    random_state=42,\n",
        "    n_estimators=100\n",
        ")\n",
        "anomaly_labels = iso_forest.fit_predict(X_scaled)\n",
        "anomaly_scores = iso_forest.score_samples(X_scaled)\n",
        "\n",
        "# Create anomaly feature (new unique feature)\n",
        "X_scaled['Anomaly_Score'] = anomaly_scores\n",
        "print(f\"    ‚úì Anomaly scores generated\")\n",
        "print(f\"    ‚úì Detected anomalies: {(anomaly_labels == -1).sum()}\")\n",
        "\n",
        "# ==================== STAGE 4: CLASS BALANCING WITH SMOTE ====================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[4] CLASS BALANCING (SMOTE)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "smote = SMOTE(random_state=42, k_neighbors=2) # Changed k_neighbors from 5 to 2\n",
        "X_balanced, y_balanced = smote.fit_resample(X_scaled, y)\n",
        "\n",
        "print(f\"    Original distribution: {y.value_counts().to_dict()}\")\n",
        "print(f\"    Balanced distribution: {pd.Series(y_balanced).value_counts().to_dict()}\")\n",
        "print(f\"    ‚úì Dataset balanced successfully\")\n",
        "\n",
        "# ==================== STAGE 5: TRAIN-TEST SPLIT ====================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[5] TRAIN-TEST SPLIT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_balanced, y_balanced,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_balanced\n",
        ")\n",
        "\n",
        "print(f\"    Training set: {X_train.shape}\")\n",
        "print(f\"    Testing set: {X_test.shape}\")\n",
        "\n",
        "# ==================== STAGE 6: MODEL TRAINING ====================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[6] MODEL TRAINING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Model 1: Logistic Regression\n",
        "print(\"\\n    Training Logistic Regression...\")\n",
        "lr_model = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "lr_model.fit(X_train, y_train)\n",
        "print(\"    ‚úì Logistic Regression trained\")\n",
        "\n",
        "# Model 2: Gradient Boosting Classifier\n",
        "print(\"    Training Gradient Boosting Classifier...\")\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "gb_model.fit(X_train, y_train)\n",
        "print(\"    ‚úì Gradient Boosting trained\")\n",
        "\n",
        "# Model 3: Random Forest (Bonus)\n",
        "print(\"    Training Random Forest Classifier...\")\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(\"    ‚úì Random Forest trained\")\n",
        "\n",
        "# ==================== STAGE 7: MODEL EVALUATION ====================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[7] MODEL EVALUATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\n    {model_name}:\")\n",
        "    print(f\"      AUC-ROC: {auc_score:.4f}\")\n",
        "    print(f\"      F1-Score: {f1:.4f}\")\n",
        "    print(f\"      Classification Report:\")\n",
        "    print(f\"      {classification_report(y_test, y_pred, digits=4)}\")\n",
        "\n",
        "    return auc_score, f1, y_pred, y_pred_proba\n",
        "\n",
        "# Evaluate all models\n",
        "lr_auc, lr_f1, lr_pred, lr_proba = evaluate_model(lr_model, X_test, y_test, \"Logistic Regression\")\n",
        "gb_auc, gb_f1, gb_pred, gb_proba = evaluate_model(gb_model, X_test, y_test, \"Gradient Boosting\")\n",
        "rf_auc, rf_f1, rf_pred, rf_proba = evaluate_model(rf_model, X_test, y_test, \"Random Forest\")\n",
        "\n",
        "# ==================== STAGE 8: CROSS-VALIDATION ====================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[8] CROSS-VALIDATION (5-Fold Stratified)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for model, name in [(lr_model, \"LR\"), (gb_model, \"GB\"), (rf_model, \"RF\")]:\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='f1')\n",
        "    print(f\"    {name} - F1 Scores: {cv_scores} | Mean: {cv_scores.mean():.4f}\")\n",
        "\n",
        "# ==================== STAGE 9: FEATURE IMPORTANCE ====================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[9] FEATURE IMPORTANCE (Top 10)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Gradient Boosting Feature Importance\n",
        "gb_importance = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': gb_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\n    Gradient Boosting Top Features:\")\n",
        "print(gb_importance.head(10).to_string(index=False))\n",
        "\n",
        "# ==================== STAGE 10: VISUALIZATION ====================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[10] GENERATING VISUALIZATIONS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# ROC Curves\n",
        "fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_proba)\n",
        "fpr_gb, tpr_gb, _ = roc_curve(y_test, gb_proba)\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_proba)\n",
        "\n",
        "axes[0, 0].plot(fpr_lr, tpr_lr, label=f'LR (AUC={lr_auc:.3f})', linewidth=2)\n",
        "axes[0, 0].plot(fpr_gb, tpr_gb, label=f'GB (AUC={gb_auc:.3f})', linewidth=2)\n",
        "axes[0, 0].plot(fpr_rf, tpr_rf, label=f'RF (AUC={rf_auc:.3f})', linewidth=2)\n",
        "axes[0, 0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "axes[0, 0].set_xlabel('False Positive Rate')\n",
        "axes[0, 0].set_ylabel('True Positive Rate')\n",
        "axes[0, 0].set_title('ROC Curves - All Models')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "# Confusion Matrices\n",
        "cm_gb = confusion_matrix(y_test, gb_pred)\n",
        "sns.heatmap(cm_gb, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Confusion Matrix - Gradient Boosting')\n",
        "axes[0, 1].set_ylabel('True Label')\n",
        "axes[0, 1].set_xlabel('Predicted Label')\n",
        "\n",
        "# Feature Importance\n",
        "top_10_features = gb_importance.head(10)\n",
        "axes[1, 0].barh(range(len(top_10_features)), top_10_features['Importance'].values)\n",
        "axes[1, 0].set_yticks(range(len(top_10_features)))\n",
        "axes[1, 0].set_yticklabels(top_10_features['Feature'].values)\n",
        "axes[1, 0].set_xlabel('Importance')\n",
        "axes[1, 0].set_title('Top 10 Feature Importance (GB)')\n",
        "axes[1, 0].invert_yaxis()\n",
        "\n",
        "# Model Comparison\n",
        "models = ['Logistic Regression', 'Gradient Boosting', 'Random Forest']\n",
        "auc_scores = [lr_auc, gb_auc, rf_auc]\n",
        "f1_scores = [lr_f1, gb_f1, rf_f1]\n",
        "\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "\n",
        "axes[1, 1].bar(x - width/2, auc_scores, width, label='AUC-ROC', alpha=0.8)\n",
        "axes[1, 1].bar(x + width/2, f1_scores, width, label='F1-Score', alpha=0.8)\n",
        "axes[1, 1].set_ylabel('Score')\n",
        "axes[1, 1].set_title('Model Comparison')\n",
        "axes[1, 1].set_xticks(x)\n",
        "axes[1, 1].set_xticklabels(models, rotation=15, ha='right')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].set_ylim([0.8, 1.0])\n",
        "axes[1, 1].grid(alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('fraud_detection_results.png', dpi=300, bbox_inches='tight')\n",
        "print(\"    ‚úì Visualizations saved as 'fraud_detection_results.png'\")\n",
        "\n",
        "# ==================== STAGE 11: SUMMARY REPORT ====================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[11] FINAL SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "summary_df = pd.DataFrame({\n",
        "    'Model': ['Logistic Regression', 'Gradient Boosting', 'Random Forest'],\n",
        "    'AUC-ROC': [lr_auc, gb_auc, rf_auc],\n",
        "    'F1-Score': [lr_f1, gb_f1, rf_f1]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + summary_df.to_string(index=False))\n",
        "\n",
        "best_model_idx = summary_df['AUC-ROC'].idxmax()\n",
        "best_model_name = summary_df.loc[best_model_idx, 'Model']\n",
        "print(f\"\\n    üèÜ Best Model: {best_model_name}\")\n",
        "print(f\"       AUC-ROC: {summary_df.loc[best_model_idx, 'AUC-ROC']:.4f}\")\n",
        "print(f\"       F1-Score: {summary_df.loc[best_model_idx, 'F1-Score']:.4f}\")\n",
        "\n",
        "# Save results to CSV\n",
        "summary_df.to_csv('model_performance_results.csv', index=False)\n",
        "print(\"\\n    ‚úì Results saved to 'model_performance_results.csv'\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PIPELINE EXECUTION COMPLETED ‚úì\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "45uQrQoSJlXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5OFNzOJXXJiW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}